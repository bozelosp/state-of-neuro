# state_of_neuro/scripts/prepare_embeddings.py
"""Step 6 of the pipeline - truncate embedding vectors and emit pickles.

Consumes JSON batches generated by :mod:`scripts.generate_embeddings`, trims
each vector to ``embed_dim`` components, and serialises grouped records to
pickle files optimised for downstream analytics (for example FAISS index
builders).

Prerequisites
-------------
- Embedding JSON batches named ``embeddings_batch_*.json`` located in
  ``input_dir``.
- ``embed_dim`` specifies the number of components to retain from each vector.
- ``block_size`` governs the number of records stored per pickle file.

Outputs
-------
- Pickle files ``batch_*.pkl`` containing truncated embeddings plus metadata.
- A :class:`EmbeddingPrepResult` instance reporting batch counts for logging.
"""

from __future__ import annotations

import json
import pickle
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List, MutableMapping, Sequence


@dataclass
class EmbeddingPrepConfig:
    """Configuration for preparing embedding batches."""

    input_dir: Path
    output_dir: Path
    embed_dim: int
    block_size: int


@dataclass
class EmbeddingPrepResult:
    """Summary of the embedding preparation run."""

    total_vectors: int
    batches_written: int
    output_files: Sequence[Path]


def run(config: EmbeddingPrepConfig) -> EmbeddingPrepResult:
    """Truncate embeddings and serialise them into pickle batches."""

    if config.embed_dim <= 0:
        raise ValueError("embed_dim must be a positive integer.")
    if config.block_size <= 0:
        raise ValueError("block_size must be a positive integer.")
    if not config.input_dir.exists():
        raise FileNotFoundError(f"Embedding input directory not found: {config.input_dir}")

    input_files = sorted(config.input_dir.glob("*.json"))
    if not input_files:
        raise FileNotFoundError(f"No embedding JSON files found in {config.input_dir}")

    config.output_dir.mkdir(parents=True, exist_ok=True)

    total_vectors = 0
    batches_written = 0
    output_files: List[Path] = []
    current_batch: List[MutableMapping[str, object]] = []

    for path in input_files:
        for record in _load_embeddings(path):
            processed = _prepare_record(record, config.embed_dim)
            current_batch.append(processed)
            total_vectors += 1
            if len(current_batch) >= config.block_size:
                batches_written += 1
                output_path = _write_batch(config.output_dir, batches_written, current_batch)
                output_files.append(output_path)
                current_batch.clear()

    if current_batch:
        batches_written += 1
        output_path = _write_batch(config.output_dir, batches_written, current_batch)
        output_files.append(output_path)
        current_batch.clear()

    return EmbeddingPrepResult(
        total_vectors=total_vectors,
        batches_written=batches_written,
        output_files=tuple(output_files),
    )


def _load_embeddings(path: Path) -> Iterable[MutableMapping[str, object]]:
    payload = json.loads(path.read_text(encoding="utf-8"))
    if not isinstance(payload, list):
        raise ValueError(f"Embedding payload must be a list: {path}")
    for index, item in enumerate(payload):
        if not isinstance(item, dict):
            raise ValueError(f"Embedding entry {index} in {path} must be an object.")
        yield dict(item)


def _prepare_record(record: MutableMapping[str, object], embed_dim: int) -> MutableMapping[str, object]:
    hash_id = record.get("hash_id")
    if not isinstance(hash_id, (str, int)):
        raise ValueError("Embedding record missing valid 'hash_id'.")
    embedding = record.get("embedding")
    if not isinstance(embedding, list):
        raise ValueError(f"Embedding for {hash_id!r} must be a list.")

    truncated = embedding[:embed_dim]
    metadata = {
        key: value
        for key, value in record.items()
        if key not in {"embedding", "hash_id"} and value is not None
    }
    return {
        "hash_id": str(hash_id),
        "embedding": truncated,
        "metadata": metadata,
    }


def _write_batch(output_dir: Path, batch_number: int, records: Iterable[MutableMapping[str, object]]) -> Path:
    batch_path = output_dir / f"batch_{batch_number:04d}.pkl"
    with batch_path.open("wb") as handle:
        pickle.dump(list(records), handle)
    return batch_path


__all__ = [
    "EmbeddingPrepConfig",
    "EmbeddingPrepResult",
    "run",
]
